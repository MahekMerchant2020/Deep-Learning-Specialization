{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Packages](#0)\n",
    "- [1 - Data synthesis: Creating a Speech Dataset](#1)\n",
    "    - [1.1 - Listening to the Data](#1-1)\n",
    "    - [1.2 - From Audio Recordings to Spectrograms](#1-2)\n",
    "    - [1.3 - Generating a Single Training Example](#1-3)\n",
    "        - [Exercise 1 - is_overlapping](#ex-1)\n",
    "        - [Exercise 2 - insert_audio_clip](#ex-2)\n",
    "        - [Exercise 3 - insert_ones](#ex-3)\n",
    "        - [Exercise 4 - create_training_example](#ex-4)\n",
    "    - [1.4 - Full Training Set](#1-4)\n",
    "    - [1.5 - Development Set](#1-5)\n",
    "- [2 - The Model](#2)\n",
    "    - [2.1 - Build the Model](#2-1)\n",
    "        - [Exercise 5 - modelf](#ex-5)\n",
    "    - [2.2 - Fit the Model](#2-2)\n",
    "    - [2.3 - Test the Model](#2-3)\n",
    "- [3 - Making Predictions](#3)\n",
    "    - [3.1 - Test on Dev Examples](#3-1)\n",
    "- [4 - Try Your Own Example! (OPTIONAL/UNGRADED)](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import IPython\n",
    "from td_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/activates/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/negatives/4.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/backgrounds/1.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use these three types of recordings (positives/negatives/backgrounds) to create a labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = graph_spectrogram(\"audio_examples/example_train.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above represents how active each frequency is (y axis) over a number of time-steps (x axis). \n",
    "\n",
    "<img src=\"images/spectrogram.png\" style=\"width:500px;height:200px;\">\n",
    "<center> **Figure 1**: Spectrogram of an audio recording </center>\n",
    "\n",
    "\n",
    "* The color in the spectrogram shows the degree to which different frequencies are present (loud) in the audio at different points in time. \n",
    "* Green means a certain frequency is more active or more present in the audio clip (louder).\n",
    "* Blue squares denote less active frequencies.\n",
    "* The dimension of the output spectrogram depends upon the hyperparameters of the spectrogram software and the length of the input. \n",
    "* In this notebook, we will be working with 10 second audio clips as the \"standard length\" for our training examples. \n",
    "    * The number of timesteps of the spectrogram will be 5511. \n",
    "    * You'll see later that the spectrogram will be the input $x$ into the network, and so $T_x = 5511$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, data = wavfile.read(\"audio_examples/example_train.wav\")\n",
    "print(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\n",
    "print(\"Time steps in input after spectrogram\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 5511 # The number of time steps input to the model from the spectrogram\n",
    "n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing into time-intervals\n",
    "Note that we may divide a 10 second interval of time with different units (steps).\n",
    "* Raw audio divides 10 seconds into 441,000 units.\n",
    "* A spectrogram divides 10 seconds into 5,511 units.\n",
    "    * $T_x = 5511$\n",
    "* You will use a Python module `pydub` to synthesize audio, and it divides 10 seconds into 10,000 units.\n",
    "* The output of our model will divide 10 seconds into 1,375 units.\n",
    "    * $T_y = 1375$\n",
    "    * For each of the 1375 time steps, the model predicts whether someone recently finished saying the trigger word \"activate\". \n",
    "* All of these are hyperparameters and can be changed (except the 441000, which is a function of the microphone). \n",
    "* We have chosen values that are within the standard range used for speech systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ty = 1375 # The number of time steps in the output of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio segments using pydub \n",
    "activates, negatives, backgrounds = load_raw_audio('./raw_data/')\n",
    "\n",
    "print(\"background len should be 10,000, since it is a 10 sec clip\\n\" + str(len(backgrounds[0])),\"\\n\")\n",
    "print(\"activate[0] len may be around 1000, since an `activate` audio clip is usually around 1 second (but varies a lot) \\n\" + str(len(activates[0])),\"\\n\")\n",
    "print(\"activate[1] len: different `activate` clips can have different lengths\\n\" + str(len(activates[1])),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_time_segment(segment_ms):\n",
    "    \"\"\"\n",
    "    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n",
    "    \n",
    "    Returns:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) in ms\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start = np.random.randint(low=0, high=10000-segment_ms)   # Make sure segment doesn't run past the 10sec background \n",
    "    segment_end = segment_start + segment_ms - 1\n",
    "    \n",
    "    return (segment_start, segment_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: is_overlapping\n",
    "\n",
    "def is_overlapping(segment_time, previous_segments):\n",
    "    \"\"\"\n",
    "    Checks if the time of a segment overlaps with the times of existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    segment_time -- a tuple of (segment_start, segment_end) for the new segment\n",
    "    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\n",
    "    \n",
    "    Returns:\n",
    "    True if the time segment overlaps with any of the existing segments, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    segment_start, segment_end = segment_time\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines)\n",
    "    # Step 1: Initialize overlap as a \"False\" flag. (≈ 1 line)\n",
    "    overlap = False\n",
    "    \n",
    "    # Step 2: loop over the previous_segments start and end times.\n",
    "    # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)\n",
    "    for previous_start, previous_end in previous_segments: # @KEEP\n",
    "        if ((segment_start <= previous_end) and (segment_end >= previous_start)):\n",
    "            overlap = True\n",
    "            break\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "def is_overlapping_test(target):\n",
    "    assert target((670, 1430), []) == False, \"Overlap with an empty list must be False\"\n",
    "    assert target((500, 1000), [(100, 499), (1001, 1100)]) == False, \"Almost overlap, but still False\"\n",
    "    assert target((750, 1250), [(100, 750), (1001, 1100)]) == True, \"Must overlap with the end of first segment\"\n",
    "    assert target((750, 1250), [(300, 600), (1250, 1500)]) == True, \"Must overlap with the begining of second segment\"\n",
    "    assert target((750, 1250), [(300, 600), (600, 1500), (1600, 1800)]) == True, \"Is contained in second segment\"\n",
    "    assert target((800, 1100), [(300, 600), (900, 1000), (1600, 1800)]) == True, \"New segment contains the second segment\"\n",
    "\n",
    "    print(\"\\033[92m All tests passed!\")\n",
    "    \n",
    "is_overlapping_test(is_overlapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\n",
    "overlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\n",
    "print(\"Overlap 1 = \", overlap1)\n",
    "print(\"Overlap 2 = \", overlap2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Overlap 1**\n",
    "        </td>\n",
    "        <td>\n",
    "           False\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Overlap 2**\n",
    "        </td>\n",
    "        <td>\n",
    "           True\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: insert_audio_clip\n",
    "\n",
    "def insert_audio_clip(background, audio_clip, previous_segments):\n",
    "    \"\"\"\n",
    "    Insert a new audio segment over the background noise at a random time step, ensuring that the \n",
    "    audio segment does not overlap with existing segments.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording.  \n",
    "    audio_clip -- the audio clip to be inserted/overlaid. \n",
    "    previous_segments -- times where audio segments have already been placed\n",
    "    \n",
    "    Returns:\n",
    "    new_background -- the updated background audio\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the duration of the audio clip in ms\n",
    "    segment_ms = len(audio_clip)\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "    # Step 1: Use one of the helper functions to pick a random time segment onto which to insert \n",
    "    # the new audio clip. (≈ 1 line)\n",
    "    segment_time = get_random_time_segment(segment_ms)\n",
    "    \n",
    "    # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep \n",
    "    # picking new segment_time at random until it doesn't overlap. To avoid an endless loop\n",
    "    # we retry 5 times(≈ 2 lines)\n",
    "    retry = 5 # @KEEP \n",
    "    while is_overlapping(segment_time, previous_segments) and retry >= 0:\n",
    "        segment_time = get_random_time_segment(segment_ms)\n",
    "        retry = retry - 1\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "        #print(segment_time)\n",
    "    # if last try is not overlaping, insert it to the background\n",
    "    if not is_overlapping(segment_time, previous_segments):\n",
    "    ### START CODE HERE ### \n",
    "        # Step 3: Append the new segment_time to the list of previous_segments (≈ 1 line)\n",
    "        previous_segments.append(segment_time)\n",
    "    ### END CODE HERE ###\n",
    "        # Step 4: Superpose audio segment and background\n",
    "        new_background = background.overlay(audio_clip, position = segment_time[0])\n",
    "    else:\n",
    "        #print(\"Timeouted\")\n",
    "        new_background = background\n",
    "        segment_time = (10000, 10000)\n",
    "    \n",
    "    return new_background, segment_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "def insert_audio_clip_test(target):\n",
    "    np.random.seed(5)\n",
    "    audio_clip, segment_time = target(backgrounds[0], activates[0], [(0, 4400)])\n",
    "    duration = segment_time[1] - segment_time[0]\n",
    "    assert segment_time[0] > 4400, \"Error: The audio clip is overlaping with the first segment\"\n",
    "    assert duration + 1 == len(activates[0]) , \"The segment length must match the audio clip length\"\n",
    "    assert audio_clip != backgrounds[0] , \"The audio clip must be different than the pure background\"\n",
    "    assert segment_time == (7286, 8201), f\"Wrong segment. Expected: Expected: (7286, 8201) got:{segment_time}\"\n",
    "\n",
    "    # Not possible to insert clip into background\n",
    "    audio_clip, segment_time = target(backgrounds[0], activates[0], [(0, 9999)])\n",
    "    assert segment_time == (10000, 10000), \"Segment must match the out by max-retry mark\"\n",
    "    assert audio_clip == backgrounds[0], \"output audio clip must be exactly the same input background\"\n",
    "\n",
    "    print(\"\\033[92m All tests passed!\")\n",
    "\n",
    "insert_audio_clip_test(insert_audio_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "audio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])\n",
    "audio_clip.export(\"insert_test.wav\", format=\"wav\")\n",
    "print(\"Segment Time: \", segment_time)\n",
    "IPython.display.Audio(\"insert_test.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Segment Time**\n",
    "        </td>\n",
    "        <td>\n",
    "           (2254, 3169)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected audio\n",
    "IPython.display.Audio(\"audio_examples/insert_reference.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: insert_ones\n",
    "\n",
    "def insert_ones(y, segment_end_ms):\n",
    "    \"\"\"\n",
    "    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment \n",
    "    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\n",
    "    50 following labels should be ones.\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "    y -- numpy array of shape (1, Ty), the labels of the training example\n",
    "    segment_end_ms -- the end time of the segment in ms\n",
    "    \n",
    "    Returns:\n",
    "    y -- updated labels\n",
    "    \"\"\"\n",
    "    _, Ty = y.shape\n",
    "    \n",
    "    # duration of the background (in terms of spectrogram time-steps)\n",
    "    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n",
    "    \n",
    "    if segment_end_y < Ty:\n",
    "        # Add 1 to the correct index in the background label (y)\n",
    "        ### START CODE HERE ### (≈ 3 lines)\n",
    "        for i in range(segment_end_y+1, segment_end_y+51):\n",
    "            if i < Ty:\n",
    "                y[0, i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "import random\n",
    "def insert_ones_test(target):\n",
    "    segment_end_y = random.randrange(0, Ty - 50) \n",
    "    segment_end_ms = int(segment_end_y * 10000.4) / Ty;    \n",
    "    arr1 = target(np.zeros((1, Ty)), segment_end_ms)\n",
    "\n",
    "    assert type(arr1) == np.ndarray, \"Wrong type. Output must be a numpy array\"\n",
    "    assert arr1.shape == (1, Ty), \"Wrong shape. It must match the input shape\"\n",
    "    assert np.sum(arr1) == 50, \"It must insert exactly 50 ones\"\n",
    "    assert arr1[0][segment_end_y - 1] == 0, f\"Array at {segment_end_y - 1} must be 0\"\n",
    "    assert arr1[0][segment_end_y] == 0, f\"Array at {segment_end_y} must be 0\"\n",
    "    assert arr1[0][segment_end_y + 1] == 1, f\"Array at {segment_end_y + 1} must be 1\"\n",
    "    assert arr1[0][segment_end_y + 50] == 1, f\"Array at {segment_end_y + 50} must be 1\"\n",
    "    assert arr1[0][segment_end_y + 51] == 0, f\"Array at {segment_end_y + 51} must be 0\"\n",
    "\n",
    "    print(\"\\033[92m All tests passed!\")\n",
    "    \n",
    "insert_ones_test(insert_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = insert_ones(np.zeros((1, Ty)), 9700)\n",
    "plt.plot(insert_ones(arr1, 4251)[0,:])\n",
    "print(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: create_training_example\n",
    "\n",
    "def create_training_example(background, activates, negatives, Ty):\n",
    "    \"\"\"\n",
    "    Creates a training example with a given background, activates, and negatives.\n",
    "    \n",
    "    Arguments:\n",
    "    background -- a 10 second background audio recording\n",
    "    activates -- a list of audio segments of the word \"activate\"\n",
    "    negatives -- a list of audio segments of random words that are not \"activate\"\n",
    "    Ty -- The number of time steps in the output\n",
    "\n",
    "    Returns:\n",
    "    x -- the spectrogram of the training example\n",
    "    y -- the label at each time step of the spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make background quieter\n",
    "    background = background - 20\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Initialize y (label vector) of zeros (≈ 1 line)\n",
    "    y = np.zeros((1, Ty))\n",
    "\n",
    "    # Step 2: Initialize segment times as empty list (≈ 1 line)\n",
    "    previous_segments = []\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n",
    "    number_of_activates = np.random.randint(0, 5)\n",
    "    random_indices = np.random.randint(len(activates), size=number_of_activates)\n",
    "    random_activates = [activates[i] for i in random_indices]\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines)\n",
    "    # Step 3: Loop over randomly selected \"activate\" clips and insert in background\n",
    "    for random_activate in random_activates: # @KEEP\n",
    "        # Insert the audio clip on the background\n",
    "        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n",
    "        # Retrieve segment_start and segment_end from segment_time\n",
    "        segment_start, segment_end = segment_time\n",
    "        # Insert labels in \"y\" at segment_end\n",
    "        y = insert_ones(y, segment_end)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n",
    "    number_of_negatives = np.random.randint(0, 3)\n",
    "    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n",
    "    random_negatives = [negatives[i] for i in random_indices]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines)\n",
    "    # Step 4: Loop over randomly selected negative clips and insert in background\n",
    "    for random_negative in random_negatives: # @KEEP\n",
    "        # Insert the audio clip on the background \n",
    "        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Standardize the volume of the audio clip \n",
    "    background = match_target_amplitude(background, -20.0)\n",
    "\n",
    "    # Export new training example \n",
    "    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n",
    "    \n",
    "    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n",
    "    x = graph_spectrogram(\"train.wav\")\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "def create_training_example_test(target):\n",
    "    np.random.seed(18)\n",
    "    x, y = target(backgrounds[0], activates, negatives, 1375)\n",
    "    \n",
    "    assert type(x) == np.ndarray, \"Wrong type for x\"\n",
    "    assert type(y) == np.ndarray, \"Wrong type for y\"\n",
    "    assert tuple(x.shape) == (101, 5511), \"Wrong shape for x\"\n",
    "    assert tuple(y.shape) == (1, 1375), \"Wrong shape for y\"\n",
    "    assert np.all(x > 0), \"All x values must be higher than 0\"\n",
    "    assert np.all(y >= 0), \"All y values must be higher or equal than 0\"\n",
    "    assert np.all(y <= 1), \"All y values must be smaller or equal than 1\"\n",
    "    assert np.sum(y) >= 50, \"It must contain at least one activate\"\n",
    "    assert np.sum(y) % 50 == 0, \"Sum of activate marks must be a multiple of 50\"\n",
    "    assert np.isclose(np.linalg.norm(x), 39745552.52075), \"Spectrogram is wrong. Check the parameters passed to the insert_audio_clip function\"\n",
    "\n",
    "    print(\"\\033[92m All tests passed!\")\n",
    "\n",
    "create_training_example_test(create_training_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(18)\n",
    "x, y = create_training_example(backgrounds[0], activates, negatives, Ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can listen to the training example you created and compare it to the spectrogram generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"train.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"audio_examples/train_reference.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can plot the associated labels for the generated training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START SKIP FOR GRADING\n",
    "np.random.seed(4543)\n",
    "nsamples = 32\n",
    "X = []\n",
    "Y = []\n",
    "for i in range(0, nsamples):\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    x, y = create_training_example(backgrounds[i % 2], activates, negatives, Ty)\n",
    "    X.append(x.swapaxes(0,1))\n",
    "    Y.append(y.swapaxes(0,1))\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "# END SKIP FOR GRADING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would like to save your dataset into a file that you can load later if you work in a more realistic environment. We let you the following code for reference. Don't try to run it into Coursera since the file system is read-only, and you cannot save files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for further uses\n",
    "# np.save(f'./XY_train/X.npy', X)\n",
    "# np.save(f'./XY_train/Y.npy', Y)\n",
    "# Load the preprocessed training examples\n",
    "# X = np.load(\"./XY_train/X.npy\")\n",
    "# Y = np.load(\"./XY_train/Y.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1-5'></a>\n",
    "### 1.5 - Development Set\n",
    "\n",
    "* To test our model, we recorded a development set of 25 examples. \n",
    "* While our training data is synthesized, we want to create a development set using the same distribution as the real inputs. \n",
    "* Thus, we recorded 25 10-second audio clips of people saying \"activate\" and other random words, and labeled them by hand. \n",
    "* This follows the principle described in Course 3 \"Structuring Machine Learning Projects\" that we should create the dev set to be as similar as possible to the test set distribution\n",
    "    * This is why our **dev set uses real audio** rather than synthesized audio. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed dev set examples\n",
    "X_dev = np.load(\"./XY_dev/X_dev.npy\")\n",
    "Y_dev = np.load(\"./XY_dev/Y_dev.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - The Model\n",
    "\n",
    "* Now that you've built a dataset, let's write and train a trigger word detection model! \n",
    "* The model will use 1-D convolutional layers, GRU layers, and dense layers. \n",
    "* Let's load the packages that will allow you to use these layers in Keras. This might take a minute to load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the model\n",
    "\n",
    "In the following model, the input of each layer is the output of the previous one. Implementing the model can be done in four steps. \n",
    "\n",
    "**Step 1**: CONV layer. Use `Conv1D()` to implement this, with 196 filters, \n",
    "a filter size of 15 (`kernel_size=15`), and stride of 4. [conv1d](https://keras.io/layers/convolutional/#conv1d)\n",
    "\n",
    "```Python\n",
    "output_x = Conv1D(filters=...,kernel_size=...,strides=...)(input_x)\n",
    "```\n",
    "* Follow this with batch normalization.  No parameters need to be set.\n",
    "\n",
    "```Python\n",
    "output_x = BatchNormalization()(input_x)\n",
    "```\n",
    "* Follow this with a ReLu activation.  Note that we can pass in the name of the desired activation as a string, all in lowercase letters.\n",
    "\n",
    "```Python\n",
    "output_x = Activation(\"...\")(input_x)\n",
    "```\n",
    "\n",
    "* Follow this with dropout, using a rate of 0.8 \n",
    "\n",
    "```Python\n",
    "output_x = Dropout(rate=...)(input_x)\n",
    "```\n",
    "\n",
    "\n",
    "**Step 2**: First GRU layer. To generate the GRU layer, use 128 units.\n",
    "```Python\n",
    "output_x = GRU(units=..., return_sequences = ...)(input_x)\n",
    "```\n",
    "* Return sequences instead of just the last time step's prediction to ensure that all the GRU's hidden states are fed to the next layer. \n",
    "* Follow this with dropout, using a rate of 0.8.\n",
    "* Follow this with batch normalization.  No parameters need to be set.\n",
    "```Python\n",
    "output_x = BatchNormalization()(input_x)\n",
    "```\n",
    "\n",
    "**Step 3**: Second GRU layer. This has the same specifications as the first GRU layer.\n",
    "* Follow this with a dropout, batch normalization, and then another dropout.\n",
    "\n",
    "**Step 4**: Create a time-distributed dense layer as follows: \n",
    "```Python\n",
    "output_x = TimeDistributed(Dense(1, activation = \"sigmoid\"))(input_x)\n",
    "```\n",
    "This creates a dense layer followed by a sigmoid, so that the parameters used for the dense layer are the same for every time step.  \n",
    "Documentation:\n",
    "* [Keras documentation on wrappers](https://keras.io/layers/wrappers/).  \n",
    "* To learn more, you can read this blog post [How to Use the TimeDistributed Layer in Keras](https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/).\n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 5 - modelf\n",
    "\n",
    "Implement `modelf()`, the architecture is presented in Figure 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C5\n",
    "# GRADED FUNCTION: modelf\n",
    "\n",
    "def modelf(input_shape):\n",
    "    \"\"\"\n",
    "    Function creating the model's graph in Keras.\n",
    "    \n",
    "    Argument:\n",
    "    input_shape -- shape of the model's input data (using Keras conventions)\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    X_input = Input(shape = input_shape)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Step 1: CONV layer (≈4 lines)\n",
    "    # Add a Conv1D with 196 units, kernel size of 15 and stride of 4\n",
    "    X = Conv1D(196, 15, strides=4)(X_input)\n",
    "    # Batch normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    # ReLu activation\n",
    "    X = Activation('relu')(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(0.8)(X)                                 \n",
    "\n",
    "    # Step 2: First GRU Layer (≈4 lines)\n",
    "    # GRU (use 128 units and return the sequences)\n",
    "    X = GRU(units = 128, return_sequences=True)(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(0.8)(X)\n",
    "    # Batch normalization.\n",
    "    X = BatchNormalization()(X)                           \n",
    "    \n",
    "    # Step 3: Second GRU Layer (≈4 lines)\n",
    "    # GRU (use 128 units and return the sequences)\n",
    "    X = GRU(units = 128, return_sequences=True)(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(0.8)(X)     \n",
    "    # Batch normalization\n",
    "    X = BatchNormalization()(X)\n",
    "    # dropout (use 0.8)\n",
    "    X = Dropout(0.8)(X)                               \n",
    "    \n",
    "    # Step 4: Time-distributed dense layer (≈1 line)\n",
    "    # TimeDistributed  with sigmoid activation \n",
    "    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT TEST\n",
    "from test_utils import *\n",
    "\n",
    "def modelf_test(target):\n",
    "    Tx = 5511\n",
    "    n_freq = 101\n",
    "    model = target(input_shape = (Tx, n_freq))\n",
    "    expected_model = [['InputLayer', [(None, 5511, 101)], 0],\n",
    "                     ['Conv1D', (None, 1375, 196), 297136, 'valid', 'linear', (4,), (15,), 'GlorotUniform'],\n",
    "                     ['BatchNormalization', (None, 1375, 196), 784],\n",
    "                     ['Activation', (None, 1375, 196), 0],\n",
    "                     ['Dropout', (None, 1375, 196), 0, 0.8],\n",
    "                     ['GRU', (None, 1375, 128), 125184, True],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['BatchNormalization', (None, 1375, 128), 512],\n",
    "                     ['GRU', (None, 1375, 128), 99072, True],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['BatchNormalization', (None, 1375, 128), 512],\n",
    "                     ['Dropout', (None, 1375, 128), 0, 0.8],\n",
    "                     ['TimeDistributed', (None, 1375, 1), 129, 'sigmoid']]\n",
    "    comparator(summary(model), expected_model)\n",
    "    \n",
    "    \n",
    "modelf_test(modelf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelf(input_shape = (Tx, n_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the model summary to keep track of the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Total params**\n",
    "        </td>\n",
    "        <td>\n",
    "           523,329\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Trainable params**\n",
    "        </td>\n",
    "        <td>\n",
    "           522,425\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Non-trainable params**\n",
    "        </td>\n",
    "        <td>\n",
    "           904\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the network is of shape (None, 1375, 1) while the input is (None, 5511, 101). The Conv1D has reduced the number of steps from 5511 to 1375. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Trigger word detection takes a long time to train. \n",
    "* To save time, we've already trained a model for about 3 hours on a GPU using the architecture you built above, and a large training set of about 4000 examples. \n",
    "* Let's load the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "json_file = open('./models/model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('./models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-2-1'></a>\n",
    "#### 2.2.1 Block Training for BatchNormalization Layers\n",
    "\n",
    "If you are going to fine-tune a pretrained model, it is important that you block the weights of all your batchnormalization layers. If you are going to train a new model from scratch, skip the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[2].trainable = False\n",
    "model.layers[7].trainable = False\n",
    "model.layers[10].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can train the model further, using the Adam optimizer and binary cross entropy loss, as follows. This will run quickly because we are training just for two epochs and with a small training set of 32 examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=1e-6, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size = 16, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Test the Model\n",
    "\n",
    "Finally, let's see how your model performs on the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc, = model.evaluate(X_dev, Y_dev)\n",
    "print(\"Dev set accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! \n",
    "* However, accuracy isn't a great metric for this task\n",
    "    * Since the labels are heavily skewed to 0's, a neural network that just outputs 0's would get slightly over 90% accuracy. \n",
    "* We could define more useful metrics such as F1 score or Precision/Recall. \n",
    "    * Let's not bother with that here, and instead just empirically see how the model does with some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Making Predictions\n",
    "\n",
    "Now that you have built a working model for trigger word detection, let's use it to make predictions. This code snippet runs audio (saved in a wav file) through the network. \n",
    "\n",
    "<!--\n",
    "can use your model to make predictions on new audio clips.\n",
    "\n",
    "You will first need to compute the predictions for an input audio clip.\n",
    "\n",
    "**Exercise**: Implement predict_activates(). You will need to do the following:\n",
    "\n",
    "1. Compute the spectrogram for the audio file\n",
    "2. Use `np.swap` and `np.expand_dims` to reshape your input to size (1, Tx, n_freqs)\n",
    "5. Use forward propagation on your model to compute the prediction at each output step\n",
    "!-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_triggerword(filename):\n",
    "    plt.subplot(2, 1, 1)\n",
    "    \n",
    "    # Correct the amplitude of the input file before prediction \n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    audio_clip = match_target_amplitude(audio_clip, -20.0)\n",
    "    file_handle = audio_clip.export(\"tmp.wav\", format=\"wav\")\n",
    "    filename = \"tmp.wav\"\n",
    "\n",
    "    x = graph_spectrogram(filename)\n",
    "    # the spectrogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n",
    "    x  = x.swapaxes(0,1)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    predictions = model.predict(x)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(predictions[0,:,0])\n",
    "    plt.ylabel('probability')\n",
    "    plt.show()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_file = \"audio_examples/chime.wav\"\n",
    "def chime_on_activate(filename, predictions, threshold):\n",
    "    audio_clip = AudioSegment.from_wav(filename)\n",
    "    chime = AudioSegment.from_wav(chime_file)\n",
    "    Ty = predictions.shape[1]\n",
    "    # Step 1: Initialize the number of consecutive output steps to 0\n",
    "    consecutive_timesteps = 0\n",
    "    # Step 2: Loop over the output steps in the y\n",
    "    for i in range(Ty):\n",
    "        # Step 3: Increment consecutive output steps\n",
    "        consecutive_timesteps += 1\n",
    "        # Step 4: If prediction is higher than the threshold and more than 20 consecutive output steps have passed\n",
    "        if consecutive_timesteps > 20:\n",
    "            # Step 5: Superpose audio and background using pydub\n",
    "            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds) * 1000)\n",
    "            # Step 6: Reset consecutive output steps to 0\n",
    "            consecutive_timesteps = 0\n",
    "        # if amplitude is smaller than the threshold reset the consecutive_timesteps counter\n",
    "        if predictions[0, i, 0] < threshold:\n",
    "            consecutive_timesteps = 0\n",
    "        \n",
    "    audio_clip.export(\"chime_output.wav\", format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "### 3.1 - Test on Dev Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore how our model performs on two unseen audio clips from the development set. Lets first listen to the two dev set clips. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./raw_data/dev/2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the model on these audio clips and see if it adds a chime after \"activate\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./raw_data/dev/1.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename  = \"./raw_data/dev/2.wav\"\n",
    "prediction = detect_triggerword(filename)\n",
    "chime_on_activate(filename, prediction, 0.5)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio to the correct format\n",
    "def preprocess_audio(filename):\n",
    "    # Trim or pad audio segment to 10000ms\n",
    "    padding = AudioSegment.silent(duration=10000)\n",
    "    segment = AudioSegment.from_wav(filename)[:10000]\n",
    "    segment = padding.overlay(segment)\n",
    "    # Set frame rate to 44100\n",
    "    segment = segment.set_frame_rate(44100)\n",
    "    # Export as wav\n",
    "    segment.export(filename, format='wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've uploaded your audio file to Coursera, put the path to your file in the variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_filename = \"audio_examples/my_audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_audio(your_filename)\n",
    "IPython.display.Audio(your_filename) # listen to the audio you uploaded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the model to predict when you say activate in the 10 second audio clip, and trigger a chime. If beeps are not being added appropriately, try to adjust the chime_threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chime_threshold = 0.5\n",
    "prediction = detect_triggerword(your_filename)\n",
    "chime_on_activate(your_filename, prediction, chime_threshold)\n",
    "IPython.display.Audio(\"./chime_output.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
